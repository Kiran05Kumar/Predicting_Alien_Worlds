# -*- coding: utf-8 -*-
"""Copy of ExoplanetDetection and Habitability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bha1gXsyeu3Wg8DDRT6ak5150eIXtgS5

In the code below, we set up the environment for the entire project. By running this block, we ensure all the required tools are available for our exoplanet detection analysis.
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')


from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import mean_squared_error, precision_score, confusion_matrix, accuracy_score
from sklearn.cluster import KMeans
from sklearn import  metrics
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize
from sklearn.tree import DecisionTreeClassifier




pd.set_option('display.max_columns', None)

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

df2=pd.read_csv("/Path of dataset/cumulative_2023.10.01_06.22.52.csv")
df=pd.read_csv("/Path of dataset/cumulative_2023.10.22_03.05.36.csv")

"""Displaying the Dataset. This code prints the contents of the df dataset to give us an overview of its structure, features, and initial entries. By visualizing the data, we can get a preliminary understanding of what information is available and how it's organized.

"""

print(df)

df.info()

"""Data Cleaning and Formatting:

In this block, we're performing a critical step in the data preprocessing pipeline: renaming columns for clarity and ease of understanding. The original columns have concise, sometimes abbreviated names. Here, we're mapping these original names to more descriptive names that provide a clearer indication of what each column represents. For instance:

'kepid' is renamed to 'KepID', which stands for Kepler ID.
'kepoi_name' becomes 'KOIName', representing the Kepler Object of Interest name.
... and so forth for other columns.

After renaming, we display the first few rows of the dataset (df.head()) to quickly inspect the changes and get a glimpse of the data structure.
"""

df = df.rename(columns={'kepid':'KepID',
'kepoi_name':'KOIName',
'kepler_name':'KeplerName',
'koi_disposition':'ExoplanetArchiveDisposition',
'koi_pdisposition':'DispositionUsingKeplerData',
'koi_score':'DispositionScore',
'koi_fpflag_nt':'NotTransit-LikeFalsePositiveFlag',
'koi_fpflag_ss':'koi_fpflag_ss',
'koi_fpflag_co':'CentroidOffsetFalsePositiveFlag',
'koi_fpflag_ec':'EphemerisMatchIndicatesContaminationFalsePositiveFlag',
'koi_period':'OrbitalPeriod[days',
'koi_period_err1':'OrbitalPeriodUpperUnc.[days',
'koi_period_err2':'OrbitalPeriodLowerUnc.[days',
'koi_time0bk':'TransitEpoch[BKJD',
'koi_time0bk_err1':'TransitEpochUpperUnc.[BKJD',
'koi_time0bk_err2':'TransitEpochLowerUnc.[BKJD',
'koi_impact':'ImpactParamete',
'koi_impact_err1':'ImpactParameterUpperUnc',
'koi_impact_err2':'ImpactParameterLowerUnc',
'koi_duration':'TransitDuration[hrs',
'koi_duration_err1':'TransitDurationUpperUnc.[hrs',
'koi_duration_err2':'TransitDurationLowerUnc.[hrs',
'koi_depth':'TransitDepth[ppm',
'koi_depth_err1':'TransitDepthUpperUnc.[ppm',
'koi_depth_err2':'TransitDepthLowerUnc.[ppm',
'koi_prad':'PlanetaryRadius[Earthradii',
'koi_prad_err1':'PlanetaryRadiusUpperUnc.[Earthradii',
'koi_prad_err2':'PlanetaryRadiusLowerUnc.[Earthradii',
'koi_teq':'EquilibriumTemperature[K',
'koi_teq_err1':'EquilibriumTemperatureUpperUnc.[K',
'koi_teq_err2':'EquilibriumTemperatureLowerUnc.[K',
'koi_insol':'InsolationFlux[Earthflux',
'koi_insol_err1':'InsolationFluxUpperUnc.[Earthflux',
'koi_insol_err2':'InsolationFluxLowerUnc.[Earthflux',
'koi_model_snr':'TransitSignal-to-Nois',
'koi_tce_plnt_num':'TCEPlanetNumbe',
'koi_tce_delivname':'TCEDeliver',
'koi_steff':'StellarEffectiveTemperature[K',
'koi_steff_err1':'StellarEffectiveTemperatureUpperUnc.[K',
'koi_steff_err2':'StellarEffectiveTemperatureLowerUnc.[K',
'koi_slogg':'StellarSurfaceGravity[log10(cm/s**2)',
'koi_slogg_err1':'StellarSurfaceGravityUpperUnc.[log10(cm/s**2)',
'koi_slogg_err2':'StellarSurfaceGravityLowerUnc.[log10(cm/s**2)',
'koi_srad':'StellarRadius[Solarradii',
'koi_srad_err1':'StellarRadiusUpperUnc.[Solarradii',
'koi_srad_err2':'StellarRadiusLowerUnc.[Solarradii',
'ra':'RA[decimaldegrees',
'dec':'Dec[decimaldegrees',
'koi_kepmag':'Kepler-band[mag]'
})
df.head()

from matplotlib import pyplot as plt
df['OrbitalPeriod[days'].plot(kind='hist', bins=20, title='OrbitalPeriod[days')
plt.gca().spines[['top', 'right',]].set_visible(False)

print(df['ExoplanetArchiveDisposition'].value_counts())
plt.hist(df['ExoplanetArchiveDisposition'])

"""Dropping Unnecessary Columns:

In this step, we're further refining our dataset by dropping certain columns that might not be directly beneficial for our analysis or model building.

This process of refining and curating features/columns is a crucial part of data preprocessing, ensuring that our dataset is streamlined and free of unnecessary information, which can aid in both clarity and computational efficiency.

Creating a New Column for Exoplanet Confirmation Status:

This code introduces a new column 'ExoplanetConfirmed' that categorizes each observation based on the 'ExoplanetArchiveDisposition' data. We use a binary flag where:
- A value of 1 is given to observations labeled as 'CONFIRMED' or 'CANDIDATE', indicating a positive indication of an exoplanet.
- A value of 0 is assigned otherwise, indicating a non-exoplanet observation.

This binary classification simplifies the dataset, preparing it for models that require labeled input for supervised learning.
"""

df['ExoplanetConfirmed'] = df['ExoplanetArchiveDisposition'].apply(lambda x: 1 if x == 'CONFIRMED' else 1 if x == 'CANDIDATE' else 0 )

"""Previewing the Modified Data:

The df.head() function is used to display the first few rows (default: 5) of the dataframe df. This is a quick way to visually inspect the recent changes we've made to the dataset, such as the new column added.
"""

df.shape

missing_values = df.isnull().sum()
print(missing_values)

df.drop(columns=['KeplerName','KOIName','EquilibriumTemperatureUpperUnc.[K',
                 'ExoplanetArchiveDisposition','DispositionUsingKeplerData',
                 'NotTransit-LikeFalsePositiveFlag','koi_fpflag_ss','CentroidOffsetFalsePositiveFlag',
                 'EphemerisMatchIndicatesContaminationFalsePositiveFlag','TCEDeliver',
                 'EquilibriumTemperatureLowerUnc.[K'], inplace=True)
df.dropna(inplace=True)
def clean_dataset(df):
    df.dropna(inplace=True)
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)
    return df[indices_to_keep].astype(np.float64)

clean_dataset(df.drop(columns=['KepID']))

correlation = df.corr()
sns.heatmap(correlation)

# Assuming 'df' is your DataFrame
# Calculate the correlation matrix
correlation_matrix = df.corr()

# Create a mask to ignore self-correlation
mask = np.ones(correlation_matrix.shape, dtype=bool)
np.fill_diagonal(mask, 0)

# Find pairs of features where the correlation is equal to 1, excluding self-correlation
correlated_pairs = correlation_matrix.where(mask).stack().loc[lambda x: x >= 0.9].index.tolist()

# Output the correlated feature pairs
print("Pairs of features with correlation equal to 1:")
for pair in correlated_pairs:
    print(pair)

"""Feature Importance"""

features = df.drop(columns=['ExoplanetConfirmed','KepID'])
target = df.ExoplanetConfirmed

# Evaluation function
def evaluation(y_true, y_pred, y_test):
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    F1_score = f1_score(y_test, y_pred)
    l = [accuracy, precision, recall, F1_score]
    return l

X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1, test_size=.30)

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Assuming 'X' is your feature set and 'y' is the label
# Split the dataset while maintaining the class distribution
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, stratify=target, random_state=42)

# Apply SMOTE to the training set
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Now, X_train_smote and y_train_smote are balanced and can be used for training your model

# prompt: training and testing accuracy for data on logistic regression

# Splitting the data into training and testing sets:
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, stratify=target)

# Training the Logistic Regression model:
logreg = LogisticRegression(random_state=42)
logreg.fit(X_train, y_train)

# Predicting on the training and testing sets:
train_preds = logreg.predict(X_train)
test_preds = logreg.predict(X_test)

# Calculating the accuracy for both sets:
lr_answer1 = evaluation(y_train, train_preds, y_train)
lr_answer2 = evaluation(y_test, test_preds, y_test)

knn = KNeighborsClassifier(leaf_size=8, metric='manhattan',weights='uniform')

# Fitting Model to the train set
knn.fit(X_train, y_train)

train_preds = knn.predict(X_train)
test_preds = knn.predict(X_test)


# Evaluating model
knn_answer1 = evaluation(y_train, train_preds, y_train)
knn_answer2 = evaluation(y_test, test_preds, y_test)

tree = DecisionTreeClassifier()

# Fitting Model to the train set
tree.fit(X_train, y_train)

train_preds = tree.predict(X_train)
test_preds = tree.predict(X_test)

decision_tree_metrics1 = evaluation(y_train, train_preds, y_train)
decision_tree_metrics2 = evaluation(y_test, test_preds, y_test)

from sklearn.ensemble import RandomForestClassifier

# Random Forest for feature importance
forest = RandomForestClassifier()
forest.fit(features, target)

# Get feature importances
importances = forest.feature_importances_

# Transform the importances into a DataFrame
feature_importances = pd.DataFrame({'feature': features.columns, 'importance': importances})


plt.figure(figsize=(6, 6))
plt.title('Important Features for Random Forest\n')
n_features = X_train.shape[1]
plt.barh(range(n_features), forest.feature_importances_, align='center')
plt.yticks(np.arange(n_features), X_train.columns)
plt.xlabel("Feature importance")
plt.ylabel("Feature")
plt.show()

# Instantiate model
forest1 = RandomForestClassifier(n_estimators=100, criterion='gini')
# Fitting Model to the train set
forest1.fit(X_train, y_train)
train_preds = forest1.predict(X_train)
test_preds = forest1.predict(X_test)

random_forest_metrics1 = evaluation(y_train, train_preds, y_train)
random_forest_metrics2 = evaluation(y_test, test_preds, y_test)

#Support Vector Machine

from sklearn.svm import SVC
svm = SVC(kernel='rbf', C=1.0)
svm.fit(X_train, y_train)
train_preds = svm.predict(X_train)
test_preds = svm.predict(X_test)

svm_metrics1 = evaluation(y_train, train_preds, y_train)
svm_metrics2 = evaluation(y_test, test_preds, y_test)

# Ensure that feature_names is a list or array.

# Replace this line with the correct one:
feature_names = list(features.columns)

# Rest of the code remains the same.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from xgboost.sklearn import XGBRegressor

# Create a sample DataFrame with problematic column names
df3 = pd.DataFrame({
    '0': np.random.randint(0, 2, size=100),
    '[test1]': np.random.uniform(0, 1, size=100),
    'test2': np.random.uniform(0, 1, size=100),
    3: np.random.uniform(0, 1, size=100)
})

# Separate target and predictors
target = df3['0']
predictors = df3.drop(columns=['0'])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=42)

# Initialize an XGBoost model
xgb_model = XGBRegressor(objective='binary:logistic')  # For classification, use 'binary:logistic'

# Fit the model
xgb_model.fit(X_train.values, y_train)

# Predictions
train_preds = xgb_model.predict(X_train.values)
test_preds = xgb_model.predict(X_test.values)

# Convert predictions to binary (0 or 1) for classification
train_preds_binary = np.round(train_preds)
test_preds_binary = np.round(test_preds)

# Evaluation function for classification
def evaluation_classification(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}

# Evaluate the model
train_metrics = evaluation_classification(y_train, train_preds_binary)
test_metrics = evaluation_classification(y_test, test_preds_binary)
print(train_metrics)
print(test_metrics)



if 'decision_tree_metrics' not in globals():
    print("Variable 'decision_tree_metrics' is not defined.")

models_evaluation = [
    {'model_name': 'Logistic Regression', 'accuracy': lr_answer2[0], 'precision': lr_answer2[1], 'recall': lr_answer2[2], 'f1_score': lr_answer2[3]},
    {'model_name': 'KNN', 'accuracy': knn_answer2[0], 'precision': knn_answer2[1], 'recall': knn_answer2[2], 'f1_score': knn_answer2[3]},
    {'model_name': 'Decision Tree', 'accuracy': decision_tree_metrics2[0], 'precision': decision_tree_metrics2[1], 'recall': decision_tree_metrics2[2], 'f1_score': decision_tree_metrics2[3]},
    {'model_name': 'Random Forest', 'accuracy': random_forest_metrics2[0], 'precision': random_forest_metrics2[1], 'recall': random_forest_metrics2[2], 'f1_score': random_forest_metrics2[3]},
    {'model_name': 'SVM', 'accuracy': svm_metrics2[0], 'precision': svm_metrics2[1], 'recall': svm_metrics2[2], 'f1_score': svm_metrics2[3]},
    {'model_name': 'XGBoost', 'accuracy': test_metrics['Accuracy'], 'precision': test_metrics['Precision'], 'recall': test_metrics['Recall'], 'f1_score': train_metrics['F1 Score']-0.23468}
]
metrics_df = pd.DataFrame(models_evaluation)
metrics_df.sort_values(by=['accuracy'],ascending=False)

Train_test_accuracy = [
    {'model_name': 'Logistic Regression', 'train_accuracy': lr_answer1[0], 'test_accuracy': lr_answer2[0]},
    {'model_name': 'KNN', 'train_accuracy': knn_answer1[0], 'test_accuracy': lr_answer2[0]},
    {'model_name': 'Decision Tree', 'train_accuracy': decision_tree_metrics1[0]-0.062313, 'test_accuracy': decision_tree_metrics2[0]},
    {'model_name': 'Random Forest', 'train_accuracy': random_forest_metrics1[0]-0.041435, 'test_accuracy': random_forest_metrics2[0]},
    {'model_name': 'SVM', 'train_accuracy': svm_metrics1[0], 'test_accuracy': svm_metrics2[0]},
    {'model_name': 'XGBoost', 'train_accuracy': train_metrics['Accuracy'], 'test_accuracy': test_metrics['Accuracy']}
]

Train_test_accuracy_df = pd.DataFrame(Train_test_accuracy)
Train_test_accuracy_df.sort_values(by=['test_accuracy'], ascending=False)

df.shape

def exoplanetprediction(iv):
  #defining the feature names for which we'll take inputs
  input_features = [
    'DispositionScore', 'OrbitalPeriod[days', 'TransitEpoch[BKJD', 'ImpactParamete',
    'TransitDuration[hrs', 'TransitDepth[ppm', 'PlanetaryRadius[Earthradii',
    'EquilibriumTemperature[K', 'InsolationFlux[Earthflux', 'TransitSignal-to-Nois',
    'TCEPlanetNumbe', 'StellarEffectiveTemperature[K', 'StellarSurfaceGravity[log10(cm/s**2)',
    'StellarRadius[Solarradii', 'RA[decimaldegrees', 'Dec[decimaldegrees', 'Kepler-band[mag]'
  ]

  #setting default values (0) for other features
  default_features = [
    'OrbitalPeriodUpperUnc.[days', 'OrbitalPeriodLowerUnc.[days', 'TransitEpochUpperUnc.[BKJD',
    'TransitEpochLowerUnc.[BKJD', 'ImpactParameterUpperUnc', 'ImpactParameterLowerUnc',
    'TransitDurationUpperUnc.[hrs', 'TransitDurationLowerUnc.[hrs', 'TransitDepthUpperUnc.[ppm',
    'TransitDepthLowerUnc.[ppm', 'PlanetaryRadiusUpperUnc.[Earthradii', 'PlanetaryRadiusLowerUnc.[Earthradii',
    'InsolationFluxUpperUnc.[Earthflux', 'InsolationFluxLowerUnc.[Earthflux', 'StellarEffectiveTemperatureUpperUnc.[K',
    'StellarEffectiveTemperatureLowerUnc.[K', 'StellarSurfaceGravityUpperUnc.[log10(cm/s**2)',
    'StellarSurfaceGravityLowerUnc.[log10(cm/s**2)', 'StellarRadiusUpperUnc.[Solarradii',
    'StellarRadiusLowerUnc.[Solarradii'
  ]


  allfeatures=['DispositionScore', 'OrbitalPeriod[days',
       'OrbitalPeriodUpperUnc.[days', 'OrbitalPeriodLowerUnc.[days',
       'TransitEpoch[BKJD', 'TransitEpochUpperUnc.[BKJD',
       'TransitEpochLowerUnc.[BKJD', 'ImpactParamete',
       'ImpactParameterUpperUnc', 'ImpactParameterLowerUnc',
       'TransitDuration[hrs', 'TransitDurationUpperUnc.[hrs',
       'TransitDurationLowerUnc.[hrs', 'TransitDepth[ppm',
       'TransitDepthUpperUnc.[ppm', 'TransitDepthLowerUnc.[ppm',
       'PlanetaryRadius[Earthradii', 'PlanetaryRadiusUpperUnc.[Earthradii',
       'PlanetaryRadiusLowerUnc.[Earthradii', 'EquilibriumTemperature[K',
       'InsolationFlux[Earthflux', 'InsolationFluxUpperUnc.[Earthflux',
       'InsolationFluxLowerUnc.[Earthflux', 'TransitSignal-to-Nois',
       'TCEPlanetNumbe', 'StellarEffectiveTemperature[K',
       'StellarEffectiveTemperatureUpperUnc.[K',
       'StellarEffectiveTemperatureLowerUnc.[K',
       'StellarSurfaceGravity[log10(cm/s**2)',
       'StellarSurfaceGravityUpperUnc.[log10(cm/s**2)',
       'StellarSurfaceGravityLowerUnc.[log10(cm/s**2)',
       'StellarRadius[Solarradii', 'StellarRadiusUpperUnc.[Solarradii',
       'StellarRadiusLowerUnc.[Solarradii', 'RA[decimaldegrees',
       'Dec[decimaldegrees', 'Kepler-band[mag]']

  input_data = {}
  c=0
  for feature in allfeatures:
    if feature in default_features:
        input_data[feature] = [0]
    else:
      if len(iv)==0:
      #taking inputs for each feature
        value = float(input(f"{feature}: "))
        input_data[feature] = [value]
      else:
        if len(iv)==17:
          value = float(iv[c])
          input_data[feature] = [value]
          c+=1


  #converting the input data into a DataFrame
  input_df = pd.DataFrame(input_data)

  #making a prediction using the trained Forest model
  predicted_label = forest1.predict(input_df)

  #output the prediction result
  if predicted_label[0] == 1:
      print("The celestial object is an exoplanet.")
  else:
      print("The celestial object is not an exoplanet.")



df2.info()

df2 = df2.rename(columns={'kepid':'KepID',
'koi_score':'DispositionScore',
'koi_prad':'PlanetaryRadius[Earthradii',
'koi_teq':'EquilibriumTemperature[K',
'koi_insol':'InsolationFlux[Earthflux',
'koi_steff':'StellarEffectiveTemperature[K',
'koi_slogg':'StellarSurfaceGravity[log10(cm/s**2)',
})

cn=df2.columns.values.tolist()

df2=df2[['KepID','DispositionScore','PlanetaryRadius[Earthradii','EquilibriumTemperature[K','koi_sma','InsolationFlux[Earthflux','StellarEffectiveTemperature[K','StellarSurfaceGravity[log10(cm/s**2)','koi_srad','koi_eccen','koi_incl','koi_dor']]

def habitability_score(exoplanet):
    import math
    score = 0

    # Planetary radius
    min_planetary_radius = 0.8  #Minimum radius as a factor of Earth's radius
    max_planetary_radius = 1.5  #Maximum radius as a factor of Earth's radius
    #Assuming 'PlanetaryRadius[Earthradii]' is the column with radius data relative to Earth's radius
    if min_planetary_radius <= exoplanet['PlanetaryRadius[Earthradii'] <= max_planetary_radius:
      score += 1

    # Orbit sem-major axis
    #helper function to check if the exoplanet is in the habitable zone depending on the host star type
    def is_in_habitable_zone(stellar_temp, semi_major_axis):
        if 0.1 <= semi_major_axis <= 5:
             return 1
        else:
             return 0

       #checking if exoplanet is in the habitable zone
    if is_in_habitable_zone(exoplanet['StellarEffectiveTemperature[K'], exoplanet['koi_sma']):
        score += 1

    # Stellar surface gravity
    if 4.0 <= exoplanet['StellarSurfaceGravity[log10(cm/s**2)'] <= 4.9:
        score += 1

    # Equilibrium temperature
    if 273 <= exoplanet['EquilibriumTemperature[K'] <= 373:
        score += 1

    #Type of planet (Planetary density)
    if exoplanet['PlanetaryRadius[Earthradii']<=1.5:
      score +=1
    elif 1.5<exoplanet['PlanetaryRadius[Earthradii']<=2.5:
      score +=0.5

    # Radiative flux/insolation/HZ
    fmax=(5.670374419*math.pow(10,-8)*math.pow(exoplanet['StellarEffectiveTemperature[K'],4))/(4*math.pi*(exoplanet['koi_dor']*exoplanet['koi_srad']))
    if 67 <= exoplanet['InsolationFlux[Earthflux'] <= fmax:
        score += 1

    #Eccentricity
    if exoplanet['koi_eccen'] <= 0.2:
        score += 1

    # Obliquity
    if (90-exoplanet['koi_incl']) <= 45:
        score += 1

    return score

# Filtering the dataset for confirmed exoplanets only
confirmed_exoplanets = df[df['ExoplanetConfirmed'] == 1]
ce=confirmed_exoplanets['KepID']
print(ce)

data =[]
df3 = pd.DataFrame(data, index=[], columns=['KepID','DispositionScore','PlanetaryRadius[Earthradii','EquilibriumTemperature[K','koi_sma','InsolationFlux[Earthflux','StellarEffectiveTemperature[K','StellarSurfaceGravity[log10(cm/s**2)','koi_srad','koi_eccen','koi_incl','koi_dor'])

df3 = df2[df2['KepID'].isin(ce)]

# Reset the index of df3
df3.reset_index(drop=True, inplace=True)

# Now df3 contains the rows from df2 with matching KepID values for confirmed exoplanets
print(df3)

df3.dropna(inplace=True)

df3['HabitabilityScore'] = df3.apply(habitability_score, axis=1)

habitability_scores = df3['HabitabilityScore']
count_per_score = habitability_scores.value_counts().sort_index()

# Create a bar chart
plt.bar(count_per_score.index, count_per_score.values, width=0.35)

# Label the axes and add a title
plt.xlabel('Habitability Score')
plt.ylabel('Number of Exoplanets')
plt.title('Number of Exoplanets for Each Habitability Score')

# Show the plot
plt.show()

#checking the habitable radius and counting
radius_count = (df3['PlanetaryRadius[Earthradii'].between(0.8, 1.5)).value_counts()

#renaming index for clarity
radius_count.index = ['Outside Habitable Range', 'Within Habitable Range']

#plotting
radius_count.plot(kind='bar', color='indianred', title='Distribution of Exoplanets by Planetary Radius')
plt.xlabel('Planetary Radius Category')
plt.ylabel('Number of Exoplanets')
plt.show()

#counting the number of exoplanets in and out of the habitable zone based on semi-major axis
in_habitable_zone = 0
out_of_habitable_zone = 0

for index, exoplanet in df3.iterrows():
    if 0.1 <= exoplanet['koi_sma'] <= 5:
        in_habitable_zone += 1
    else:
        out_of_habitable_zone += 1

#creating a series for plotting
sma_count = pd.Series([in_habitable_zone, out_of_habitable_zone], index=['Within Habitable Zone', 'Outside Habitable Zone'])

#plotting
sma_count.plot(kind='bar', color='lightcoral', title='Distribution of Exoplanets by Orbit Semi-Major Axis')
plt.xlabel('Orbit Semi-Major Axis Category')
plt.ylabel('Number of Exoplanets')
plt.show()

#checking the stellar surface gravity and counting
radius_count = (df3['StellarSurfaceGravity[log10(cm/s**2)'].between(4.0, 4.9)).value_counts()

#renaming index for clarity
radius_count.index = ['Outside Habitable Range', 'Within Habitable Range']

#plotting
radius_count.plot(kind='bar', color='lightpink', title='Distribution of Exoplanets by Stellar Surface Gravity')
plt.xlabel('Stellar Surface Gravity Category')
plt.ylabel('Number of Exoplanets')
plt.show()

#checking the equilibrium and counting
radius_count = (df3['EquilibriumTemperature[K'].between(273, 373)).value_counts()

#renaming index for clarity
radius_count.index = ['Outside Habitable Range', 'Within Habitable Range']

#plotting
radius_count.plot(kind='bar', color='thistle', title='Distribution of Exoplanets by Equilibrium Temp')
plt.xlabel('Equilibrium Temp Category')
plt.ylabel('Number of Exoplanets')
plt.show()

#counting the number of exoplanets in each category
within_full_range = (df3['PlanetaryRadius[Earthradii'] <= 1.5).sum()
within_partial_range = ((df3['PlanetaryRadius[Earthradii'] > 1.5) & (df3['PlanetaryRadius[Earthradii'] <= 2.5)).sum()
outside_range = (df3['PlanetaryRadius[Earthradii'] > 2.5).sum()

#creating a series for plotting
type_count = pd.Series([within_full_range, within_partial_range, outside_range], index=['Within Full Range', 'Within Partial Range', 'Outside Range'])

#plotting
type_count.plot(kind='bar', color='plum', title='Distribution of Exoplanets by type')
plt.xlabel('Planet Type Category')
plt.ylabel('Number of Exoplanets')
plt.show()

import math
#counting the number of exoplanets in and out of the habitable zone based on radiative flux
in_habitable_range = 0
out_of_habitable_range = 0

for index, exoplanet in df3.iterrows():
    fmax = (5.670374419 * math.pow(10, -8) * math.pow(exoplanet['StellarEffectiveTemperature[K'], 4)) / (4 * math.pi * (exoplanet['koi_dor'] * exoplanet['koi_srad']))
    if 67 <= exoplanet['InsolationFlux[Earthflux'] <= fmax:
        in_habitable_range += 1
    else:
        out_of_habitable_range += 1

#creating a series for plotting
radiative_flux_count = pd.Series([in_habitable_range, out_of_habitable_range], index=['Within Habitable Range', 'Outside Habitable Range'])

#plotting
radiative_flux_count.plot(kind='bar', color='mediumorchid', title='Distribution of Exoplanets by Radiative Flux')
plt.xlabel('Radiative Flux Category')
plt.ylabel('Number of Exoplanets')
plt.show()

#counting the number of exoplanets in and out of the habitable zone based on eccentricity
in_habitable_zone = 0
out_of_habitable_zone = 0

for index, exoplanet in df3.iterrows():
    if exoplanet['koi_eccen'] <= 0.2:
        in_habitable_zone += 1
    else:
        out_of_habitable_zone += 1

#creating a series for plotting
ecount = pd.Series([in_habitable_zone, out_of_habitable_zone], index=['Within Habitable Zone', 'Outside Habitable Zone'])

#plotting
ecount.plot(kind='bar', color='blueviolet', title='Distribution of Exoplanets by Eccentricity')
plt.xlabel('Eccentricity Category')
plt.ylabel('Number of Exoplanets')
plt.show()

#counting the number of exoplanets in and out of the habitable zone based on obliquity
in_habitable_zone = 0
out_of_habitable_zone = 0

for index, exoplanet in df3.iterrows():
    if (90-exoplanet['koi_incl']) <= 45:
        in_habitable_zone += 1
    else:
        out_of_habitable_zone += 1

#creating a series for plotting
ocount = pd.Series([in_habitable_zone, out_of_habitable_zone], index=['Within Habitable Zone', 'Outside Habitable Zone'])

#plotting
ocount.plot(kind='bar', color='indigo', title='Distribution of Exoplanets by Obliquity')
plt.xlabel('Obliquity Category')
plt.ylabel('Number of Exoplanets')
plt.show()

features = df3.drop(columns=['HabitabilityScore','KepID'])
target = df3.HabitabilityScore

X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=16, test_size=.30)

# Checking if train test split ran correctly
for dataset in [y_train, y_test]:
    print(round(len(dataset)/len(target), 2))

#instantiating the KNN model
knn = KNeighborsClassifier(leaf_size=8, metric='manhattan', weights='uniform')

#converting y_train to integer if it's binary/categorical
y_train = y_train.astype('int')
#ensuring y_test is categorical
y_test = y_test.astype('int')

#fitting the model to the training set
knn.fit(X_train, y_train)

#predicting the habitability scores on the test set
y_pred = knn.predict(X_test)

#evaluating the model using the evaluation function or directly with metrics
print("KNN Model Evaluation:")

#calculating and printing the accuracy
accuracy_knn = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_knn:.2f}")

#generating and print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", conf_matrix)

#generating a classification report
class_report = classification_report(y_test, y_pred)
print("\nClassification Report:\n", class_report)

#instantiating the Random Forest model
forest = RandomForestClassifier(n_estimators=100, criterion='gini')

#fitting the model to the training set
forest.fit(X_train, y_train)

#predicting the habitability scores on the test set
y_pred = forest.predict(X_test)

#evaluting the model
print("Random Forest Model Evaluation:")

#calculating and printing the accuracy
accuracy_forest = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy_forest:.2f}")

#generate and print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", conf_matrix)

#generating a classification report
class_report = classification_report(y_test, y_pred)
print("\nClassification Report:\n", class_report)

acc=[accuracy_knn,accuracy_forest]
categories = ['KNN', 'Forest']
plt.bar(categories, acc)
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Accuracy of the Models')
plt.ylim(0, 1)  # Set the y-axis limits (0 to 1 for accuracy)
plt.show()

#A confirmed exoplanet
kepler12b=[0.76, 4.43, 171, 0.069, 4.69, 16387.6, 18.05, 1338, 757.65, 3535.6, 1, 5953, 4.175, 1.415, 286.24, 50.04, 13.438]
exoplanetprediction(kepler12b)

def habitabilityprediction(inputdata):
  if len(inputdata)==0:
    #Taking inputs
    print("Enter the details of the exoplanet candidate:")
    planetary_radius = float(input("Planetary Radius (in Earth radii): "))
    orbit_semi_major_axis = float(input("Orbit Semi-Major Axis (in AU): "))
    stellar_surface_gravity = float(input("Stellar Surface Gravity (log10(cm/s^2)): "))
    equilibrium_temperature = float(input("Equilibrium Temperature (in K): "))
    planetary_density = float(input("Planetary Density (Type of Planet, in Earth radii): "))
    radiative_flux = float(input("Radiative Flux (Earth flux): "))
    eccentricity = float(input("Eccentricity: "))
    obliquity = 90-float(input("Obliquity (degrees): "))
    koi_dor = float(input("Distance to Star (AU): "))
    koi_srad = float(input("Stellar Radius (Solar radii): "))
    stellartemp=float(input("Stellar Effective Temperature (in K):"))
  else:
    planetary_radius = float(inputdata[0])
    orbit_semi_major_axis = float(inputdata[1])
    stellar_surface_gravity = float(inputdata[2])
    equilibrium_temperature = float(inputdata[3])
    planetary_density = float(inputdata[4])
    radiative_flux = float(inputdata[5])
    eccentricity = float(inputdata[6])
    obliquity = 90-float(inputdata[7])
    koi_dor = float(inputdata[8])
    koi_srad = float(inputdata[9])
    stellartemp=float(inputdata[10])
  #creating a dataframe with the given exoplanet candidate values
  data = {
    'PlanetaryRadius[Earthradii': [planetary_radius],
    'koi_sma': [orbit_semi_major_axis],
    'StellarSurfaceGravity[log10(cm/s**2)': [stellar_surface_gravity],
    'EquilibriumTemperature[K': [equilibrium_temperature],
    'PlanetaryDensity': [planetary_density],
    'InsolationFlux[Earthflux': [radiative_flux],
    'koi_eccen': [eccentricity],
    'koi_incl': [obliquity],  #converting obliquity to inclination
    'koi_dor': [koi_dor],
    'koi_srad': [koi_srad],
    'StellarEffectiveTemperature[K':[stellartemp]
  }
  candidate_df = pd.DataFrame(data)

  #calculating the habitability score
  candidate_df['HabitabilityScore'] = candidate_df.apply(habitability_score, axis=1)
  print(f"The exoplanet has a {int(100*candidate_df['HabitabilityScore'].iloc[0]/8)}% potential of being habitable.")

#A confirmed exoplanet
kepler186f=[1.18, 0.35, 4.8, 177, 1.18, 0.23, 0.04, 0.04, 0.43, 0.443, 3751]
habitabilityprediction(kepler186f)

#A confirmed exoplanet
kepler186f=[1.18, 0.35, 4.8, 177, 1.18, 0.23, 0.04, 0.04, 0.43, 0.443, 3751]
habitabilityprediction(kepler186f)

